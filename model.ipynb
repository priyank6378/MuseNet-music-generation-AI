{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_nlp\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import music21\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTES_VOCAB_SIZE = 200\n",
    "DURATION_VOCAB_SIZE = 200\n",
    "MAX_LEN = 50\n",
    "EMBEDDING_DIM = 128\n",
    "KEY_DIM = 256\n",
    "N_HEADS = 2\n",
    "FEED_FORWARD_DIM = 256\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "GENERATE_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset preparation\n",
    "notes_list = []\n",
    "duration_list = []\n",
    "notes = []\n",
    "durations = []\n",
    "parser =  music21.converter\n",
    "file_list = glob.glob(\"bach_cello_suite_data/*.mid\")\n",
    "for i, file in enumerate(file_list):\n",
    "    print(i + 1, \"Parsing %s\" % file)\n",
    "    score = parser.parse(file).chordify()\n",
    "\n",
    "    notes.append(\"START\")\n",
    "    durations.append(\"0.0\")\n",
    "\n",
    "    for element in score.flat:\n",
    "        note_name = None\n",
    "        duration_name = None\n",
    "\n",
    "        if isinstance(element, music21.key.Key):\n",
    "            note_name = str(element.tonic.name) + \":\" + str(element.mode)\n",
    "            duration_name = \"0.0\"\n",
    "\n",
    "        elif isinstance(element, music21.meter.TimeSignature):\n",
    "            note_name = str(element.ratioString) + \"TS\"\n",
    "            duration_name = \"0.0\"\n",
    "\n",
    "        elif isinstance(element, music21.chord.Chord):\n",
    "            note_name = element.pitches[-1].nameWithOctave\n",
    "            duration_name = str(element.duration.quarterLength)\n",
    "\n",
    "        elif isinstance(element, music21.note.Rest):\n",
    "            note_name = str(element.name)\n",
    "            duration_name = str(element.duration.quarterLength)\n",
    "\n",
    "        elif isinstance(element, music21.note.Note):\n",
    "            note_name = str(element.nameWithOctave)\n",
    "            duration_name = str(element.duration.quarterLength)\n",
    "\n",
    "        if note_name and duration_name:\n",
    "            notes.append(note_name)\n",
    "            durations.append(duration_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_list = []\n",
    "duration_list = []\n",
    "\n",
    "for i in range(len(notes) - MAX_LEN - 1):\n",
    "    notes_list.append(\" \".join(notes[i : (i + MAX_LEN + 1)]))\n",
    "    duration_list.append(\" \".join(durations[i : (i + MAX_LEN + 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(notes_list[658])\n",
    "print(duration_list[658])\n",
    "print(len(notes))\n",
    "print(len(durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes vectorization layer\n",
    "notes_ds = tf.data.Dataset.from_tensor_slices(notes_list).batch(BATCH_SIZE, drop_remainder=True).shuffle(1000)\n",
    "notes_vec_layer = layers.TextVectorization(\n",
    "    output_mode='int',\n",
    "    standardize=None\n",
    ")\n",
    "notes_vec_layer.adapt(notes_ds)\n",
    "NOTES_VOCAB_SIZE = len(notes_vec_layer.get_vocabulary())\n",
    "\n",
    "# duration vectorization layer\n",
    "duration_ds = tf.data.Dataset.from_tensor_slices(duration_list).batch(BATCH_SIZE, drop_remainder=True).shuffle(1000)\n",
    "duration_vec_layer = layers.TextVectorization(\n",
    "    output_mode='int',\n",
    "    standardize=None\n",
    ")\n",
    "duration_vec_layer.adapt(duration_ds)\n",
    "DURATION_VOCAB_SIZE = len(duration_vec_layer.get_vocabulary())\n",
    "\n",
    "# input_ds\n",
    "input_ds = tf.data.Dataset.zip((notes_ds, duration_ds))\n",
    "\n",
    "def prepare_dataset(notes, durations):\n",
    "    notes = tf.expand_dims(notes, -1)\n",
    "    durations = tf.expand_dims(durations,-1)\n",
    "    tokenized_notes = notes_vec_layer(notes)\n",
    "    tokenized_durations = duration_vec_layer(durations)\n",
    "    x = (tokenized_notes[:, :-1], tokenized_durations[:, :-1])\n",
    "    y = (tokenized_notes[:, 1:], tokenized_durations[:, 1:])\n",
    "    return x, y\n",
    "\n",
    "full_ds = input_ds.map(prepare_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in full_ds.take(1):\n",
    "    print(i[0][0].shape, i[0][1].shape, i[1][0].shape, i[1][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cusal attention : \n",
    "\n",
    "def causal_attention_mask(batch_size, n_dest , n_src, dtype):\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1,1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "np.transpose(causal_attention_mask(1, 10, 10, dtype=tf.int32)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer block \n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1, name=\"transformer_block\"):\n",
    "        super(TransformerBlock, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.attn = layers.MultiHeadAttention(num_heads, key_dim, output_shape=embed_dim)\n",
    "        self.dropout1 = layers.Dropout(self.dropout_rate)\n",
    "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn1 = layers.Dense(self.ff_dim, activation='relu')\n",
    "        self.ffn2 = layers.Dense(self.embed_dim)\n",
    "        self.dropout2 = layers.Dropout(self.dropout_rate)\n",
    "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(\n",
    "            batch_size, seq_len, seq_len, tf.bool\n",
    "        ) \n",
    "        attention_output, attention_scores = self.attn(\n",
    "            inputs,\n",
    "            inputs,\n",
    "            attention_mask=causal_mask,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.ln1(inputs + attention_output)\n",
    "        ffn1 = self.ffn1(out1)\n",
    "        ffn2 = self.ffn2(ffn1)\n",
    "        ffn_out = self.dropout2(ffn2)\n",
    "        return ( self.ln2(out1 + ffn_out), attention_scores )\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"key_dim\": self.key_dim,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"ff_dim\": self.ff_dim,\n",
    "                \"dropout_rate\": self.dropout_rate,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token and position embedding\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.token_embed = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_embed = keras_nlp.layers.SinePositionEncoding()\n",
    "\n",
    "    def call(self, x):\n",
    "        embeddgins = self.token_embed(x)\n",
    "        positions = self.pos_embed(embeddgins)\n",
    "        return embeddgins + positions\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building model\n",
    "\n",
    "note_inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "duration_inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "note_embed = TokenAndPositionEmbedding(NOTES_VOCAB_SIZE, EMBEDDING_DIM//2)(note_inputs)\n",
    "duration_embed = TokenAndPositionEmbedding(DURATION_VOCAB_SIZE, EMBEDDING_DIM//2)(duration_inputs)\n",
    "x = layers.Concatenate()([note_embed, duration_embed])\n",
    "x, attention_scores = TransformerBlock(N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM, name='attention')(x)\n",
    "\n",
    "notes_outputs = layers.Dense(NOTES_VOCAB_SIZE, activation='softmax')(x)\n",
    "duration_outputs = layers.Dense(DURATION_VOCAB_SIZE, activation='softmax')(x)\n",
    "\n",
    "\n",
    "museNet = keras.Model(inputs=[note_inputs, duration_inputs], outputs=[notes_outputs, duration_outputs])\n",
    "museNet.compile(\"adam\", loss=[tf.keras.losses.SparseCategoricalCrossentropy(), tf.keras.losses.SparseCategoricalCrossentropy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    museNet = tf.keras.models.load_model(\"museNet\", compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "from fractions import Fraction\n",
    "import time\n",
    "\n",
    "class SaveModelCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.save('checkpoint/MuseNet.keras')\n",
    "\n",
    "def get_midi_note(sample_note, sample_duration):\n",
    "    new_note = None\n",
    "\n",
    "    if \"TS\" in sample_note:\n",
    "        new_note = music21.meter.TimeSignature(sample_note.split(\"TS\")[0])\n",
    "\n",
    "    elif \"major\" in sample_note or \"minor\" in sample_note:\n",
    "        tonic, mode = sample_note.split(\":\")\n",
    "        new_note = music21.key.Key(tonic, mode)\n",
    "\n",
    "    elif sample_note == \"rest\":\n",
    "        new_note = music21.note.Rest()\n",
    "        new_note.duration = music21.duration.Duration(\n",
    "            float(Fraction(sample_duration))\n",
    "        )\n",
    "        new_note.storedInstrument = music21.instrument.Violoncello()\n",
    "\n",
    "    elif \".\" in sample_note:\n",
    "        notes_in_chord = sample_note.split(\".\")\n",
    "        chord_notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            n = music21.note.Note(current_note)\n",
    "            n.duration = music21.duration.Duration(\n",
    "                float(Fraction(sample_duration))\n",
    "            )\n",
    "            n.storedInstrument = music21.instrument.Violoncello()\n",
    "            chord_notes.append(n)\n",
    "        new_note = music21.chord.Chord(chord_notes)\n",
    "\n",
    "    elif sample_note == \"rest\":\n",
    "        new_note = music21.note.Rest()\n",
    "        new_note.duration = music21.duration.Duration(\n",
    "            float(Fraction(sample_duration))\n",
    "        )\n",
    "        new_note.storedInstrument = music21.instrument.Violoncello()\n",
    "\n",
    "    elif sample_note != \"START\":\n",
    "        new_note = music21.note.Note(sample_note)\n",
    "        new_note.duration = music21.duration.Duration(\n",
    "            float(Fraction(sample_duration))\n",
    "        )\n",
    "        new_note.storedInstrument = music21.instrument.Violoncello()\n",
    "\n",
    "    return new_note\n",
    "\n",
    "\n",
    "class MusicGenerator(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, index_to_note, index_to_duration, top_k=10):\n",
    "        self.index_to_note = index_to_note\n",
    "        self.note_to_index = {\n",
    "            note: index for index, note in enumerate(index_to_note)\n",
    "        }\n",
    "        self.index_to_duration = index_to_duration\n",
    "        self.duration_to_index = {\n",
    "            duration: index for index, duration in enumerate(index_to_duration)\n",
    "        }\n",
    "\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(len(probs), p=probs), probs\n",
    "\n",
    "    def get_note(self, notes, durations, temperature):\n",
    "        sample_note_idx = 1\n",
    "        while sample_note_idx == 1:\n",
    "            sample_note_idx, note_probs = self.sample_from(\n",
    "                notes[0][-1], temperature\n",
    "            )\n",
    "            sample_note = self.index_to_note[sample_note_idx]\n",
    "\n",
    "        sample_duration_idx = 1\n",
    "        while sample_duration_idx == 1:\n",
    "            sample_duration_idx, duration_probs = self.sample_from(\n",
    "                durations[0][-1], temperature\n",
    "            )\n",
    "            sample_duration = self.index_to_duration[sample_duration_idx]\n",
    "\n",
    "        new_note = get_midi_note(sample_note, sample_duration)\n",
    "\n",
    "        return (\n",
    "            new_note,\n",
    "            sample_note_idx,\n",
    "            sample_note,\n",
    "            note_probs,\n",
    "            sample_duration_idx,\n",
    "            sample_duration,\n",
    "            duration_probs,\n",
    "        )\n",
    "\n",
    "    def generate(self, start_notes, start_durations, max_tokens, temperature):\n",
    "        attention_model = keras.models.Model(\n",
    "            inputs=self.model.input,\n",
    "            outputs=self.model.get_layer(\"attention\").output,\n",
    "        )\n",
    "\n",
    "        start_note_tokens = [self.note_to_index.get(x, 1) for x in start_notes]\n",
    "        start_duration_tokens = [\n",
    "            self.duration_to_index.get(x, 1) for x in start_durations\n",
    "        ]\n",
    "        sample_note = None\n",
    "        sample_duration = None\n",
    "        info = []\n",
    "        midi_stream = music21.stream.Stream()\n",
    "\n",
    "        midi_stream.append(music21.clef.BassClef())\n",
    "\n",
    "        for sample_note, sample_duration in zip(start_notes, start_durations):\n",
    "            new_note = get_midi_note(sample_note, sample_duration)\n",
    "            if new_note is not None:\n",
    "                midi_stream.append(new_note)\n",
    "\n",
    "        while len(start_note_tokens) < max_tokens:\n",
    "            x1 = np.array([start_note_tokens])\n",
    "            x2 = np.array([start_duration_tokens])\n",
    "            notes, durations = self.model.predict([x1, x2], verbose=0)\n",
    "\n",
    "            repeat = True\n",
    "\n",
    "            while repeat:\n",
    "                (\n",
    "                    new_note,\n",
    "                    sample_note_idx,\n",
    "                    sample_note,\n",
    "                    note_probs,\n",
    "                    sample_duration_idx,\n",
    "                    sample_duration,\n",
    "                    duration_probs,\n",
    "                ) = self.get_note(notes, durations, temperature)\n",
    "\n",
    "                if (\n",
    "                    isinstance(new_note, music21.chord.Chord)\n",
    "                    or isinstance(new_note, music21.note.Note)\n",
    "                    or isinstance(new_note, music21.note.Rest)\n",
    "                ) and sample_duration == \"0.0\":\n",
    "                    repeat = True\n",
    "                else:\n",
    "                    repeat = False\n",
    "\n",
    "            if new_note is not None:\n",
    "                midi_stream.append(new_note)\n",
    "\n",
    "            _, att = attention_model.predict([x1, x2], verbose=0)\n",
    "\n",
    "            info.append(\n",
    "                {\n",
    "                    \"prompt\": [start_notes.copy(), start_durations.copy()],\n",
    "                    \"midi\": midi_stream,\n",
    "                    \"chosen_note\": (sample_note, sample_duration),\n",
    "                    \"note_probs\": note_probs,\n",
    "                    \"duration_probs\": duration_probs,\n",
    "                    \"atts\": att[0, :, -1, :],\n",
    "                }\n",
    "            )\n",
    "            start_note_tokens.append(sample_note_idx)\n",
    "            start_duration_tokens.append(sample_duration_idx)\n",
    "            start_notes.append(sample_note)\n",
    "            start_durations.append(sample_duration)\n",
    "\n",
    "            if sample_note == \"START\":\n",
    "                break\n",
    "    \n",
    "        return info\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        info = self.generate(\n",
    "            [\"START\"], [\"0.0\"], max_tokens=200, temperature=0.7\n",
    "        )\n",
    "        midi_stream = info[-1][\"midi\"].chordify()\n",
    "        print(info[-1][\"prompt\"])\n",
    "        midi_stream.write(\n",
    "            \"midi\",\n",
    "            fp=os.path.join(\n",
    "                \"outputs/\",\n",
    "                \"output-\" + str(epoch).zfill(4) + \".mid\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "callbacks = [SaveModelCallback(), MusicGenerator(notes_vec_layer.get_vocabulary(), duration_vec_layer.get_vocabulary())]\n",
    "\n",
    "# text_generator = TextGenerator(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "museNet.fit(\n",
    "    full_ds,\n",
    "    epochs=2,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
